{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "colab": {
   "name": "cmsc472_spring_21_hw4.ipynb",
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "othvD7ipIvpa"
   },
   "source": [
    "# Basic Instructions\n",
    "\n",
    "1. Enter your Name, UID and Link to Google Drive in the provided space.\n",
    "2. Submit the assignment to Gradescope.\n",
    "\n",
    "Intermediate Submission Deadline: April 13, 5:00pm\n",
    "\n",
    "Final Submission Deadline: April 16, 5:00pm\n",
    "\n",
    "As before, submit your challenge file to ELMS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odgPj7m0Ivpe"
   },
   "source": [
    "Name:  **Siyuan Peng**\n",
    "UID:  **116243407**\n",
    "\n",
    "Link to Google Drive : **Link Here (make sure it works)**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZDgYhuc1Ivpf"
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLyZgTMxIvpg"
   },
   "source": [
    "## Dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-6U-NEOGHLk"
   },
   "source": [
    "For this assignment, we will use the following dataset, which contains images of animals and such with segmentations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LEw35Ovj2WX8"
   },
   "source": [
    "#For deleting the dataset\n",
    "#!rm -r SegmentationDataset/"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96OvEtw6FyyQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#Use this to download if not using colab\n",
    "download_link='https://drive.google.com/file/d/1vDWwIBXcZURKsKwQUhGmFdgqwBhyIaIn/view?usp=sharing'\n",
    "\n",
    "#If using colab dataset can be downloaded using this command\n",
    "!gdown --id 1vDWwIBXcZURKsKwQUhGmFdgqwBhyIaIn\n",
    "!unzip --qq SegmentationDataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7W8WrUf0F4d5"
   },
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import os\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import _pickle as pickle\n",
    "import torchvision.models as models\n",
    "import glob\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "# msrc_directory = current_directory + '/SegmentationDataset'\n",
    "msrc_directory = 'SegmentationDataset'\n",
    "os.chdir('E:\\Study\\cmsc472\\CMSC472_hw4')\n",
    "print(f'Current Directory {os.getcwd()}')\n",
    "def plot_image(im,title,xticks=[],yticks= [],cv2 = True):\n",
    "    \"\"\"\n",
    "    im :Image to plot\n",
    "    title : Title of image \n",
    "    xticks : List of tick values. Defaults to nothing\n",
    "    yticks :List of tick values. Defaults to nothing \n",
    "    cv2 :Is the image cv2 image? cv2 images are BGR instead of RGB. Default True\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure()\n",
    "    if(im.shape[2]==1):\n",
    "        plt.imshow(np.squeeze(im),cmap='gray')\n",
    "    elif cv2:\n",
    "        plt.imshow(im[:,:,::-1])\n",
    "    else:\n",
    "        plt.imshow(im)\n",
    "    plt.title(title)\n",
    "    plt.xticks(xticks)\n",
    "    plt.yticks(yticks)\n",
    "    "
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory E:\\Study\\cmsc472\\CMSC472_hw4\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fWmzgdJrzBGN"
   },
   "source": [
    "# from Dataset_v1\n",
    "SEG_LABELS_LIST_v1 = [\n",
    "    {\"id\": -1, \"name\": \"void\",       \"rgb_values\": [0,   0,    0]},\n",
    "    {\"id\": 0,  \"name\": \"building\",   \"rgb_values\": [128, 0,    0]},\n",
    "    {\"id\": 1,  \"name\": \"grass\",      \"rgb_values\": [0,   128,  0]},\n",
    "    {\"id\": 2,  \"name\": \"tree\",       \"rgb_values\": [128, 128,  0]},\n",
    "    {\"id\": 3,  \"name\": \"cow\",        \"rgb_values\": [0,   0,    128]},\n",
    "    {\"id\": 4,  \"name\": \"sky\",        \"rgb_values\": [128, 128,  128]},\n",
    "    {\"id\": 5,  \"name\": \"airplane\",   \"rgb_values\": [192, 0,    0]},\n",
    "    {\"id\": 6, \"name\": \"face\",       \"rgb_values\": [192, 128,  0]},\n",
    "    {\"id\": 7, \"name\": \"car\",        \"rgb_values\": [64,  0,    128]},\n",
    "    {\"id\": 8, \"name\": \"bicycle\",    \"rgb_values\": [192, 0,    128]}]\n",
    "\n",
    "background_classes=[\"void\",\"grass\",\"sky\"]\n",
    "background_colors=[]\n",
    "for i in range(len(SEG_LABELS_LIST_v1)):\n",
    "    if SEG_LABELS_LIST_v1[i][\"name\"] in background_classes:\n",
    "        background_colors.append(SEG_LABELS_LIST_v1[i][\"rgb_values\"])\n",
    "\n",
    "\n",
    "\n",
    "def get_binary_seg(bgr_seg):\n",
    "    rgb_seg=bgr_seg#[:,:,::-1]#reverse order of channels from bgr to rgb\n",
    "    shape_rgb=rgb_seg.shape\n",
    "    binary_shape=(shape_rgb[0],shape_rgb[1],1)\n",
    "\n",
    "    binary_map=np.ones( binary_shape )\n",
    "    for background_color in background_colors:\n",
    "        binary_map[(rgb_seg==background_color).all(2)]=0\n",
    "    \n",
    "    return binary_map"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLsf4PLUGQJr"
   },
   "source": [
    "Here are some examples."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 804
    },
    "id": "4XahODT5F4zR",
    "outputId": "09758b6b-17be-46b3-96db-0079fffa3a60",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "#  plot a sample image and its ground truth segments\n",
    "image_sample = cv2.imread('SegmentationDataset/train/1_19_s.bmp')\n",
    "seg_sample = cv2.imread('SegmentationDataset/train/1_19_s_GT.bmp') \n",
    "print(type(seg_sample))\n",
    "print(seg_sample.shape)\n",
    "plot_image(image_sample, 'image')\n",
    "plot_image(seg_sample, 'seg')\n",
    "plot_image(get_binary_seg(seg_sample), 'binary_seg')"
   ],
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(213, 320, 3)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WqSCxnOGTwU"
   },
   "source": [
    "Here we provide you with a Dataset and dataloaders.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8NPds8PiF6M1",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "class SegmentationData(data.Dataset):\n",
    "    #168:48:24 split\n",
    "    def __init__(self, transform, mode='train'):\n",
    "        if mode not in ['train','test','val']:\n",
    "            raise ValueError('Invalid Split %s' % mode)\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.img_list_train_val = [x.split('.')[-2].split('/')[-1][:-3] for x in glob.glob(msrc_directory+'/train/*') if 'GT' in x]\n",
    "        self.img_list_train_val.sort()\n",
    "        self.img_list_test = [x.split('.')[-2].split('/')[-1] for x in glob.glob(msrc_directory+'/test/*')]\n",
    "        self.img_list_test.sort()\n",
    "\n",
    "        self.x={}\n",
    "        self.y={}\n",
    "        self.x['train'] = ['%s/%s.bmp' %(msrc_directory+'/train',x) for x in self.img_list_train_val[:168]]\n",
    "        self.y['train'] = ['%s/%s_GT.bmp' %(msrc_directory+'/train',x) for x in self.img_list_train_val[:168]]\n",
    "        self.x['val'] = ['%s/%s.bmp' %(msrc_directory+'/train',x) for x in self.img_list_train_val[168:]]\n",
    "        self.y['val'] = ['%s/%s_GT.bmp' %(msrc_directory+'/train',x) for x in self.img_list_train_val[168:]]\n",
    "        self.x['test'] = ['%s/%s.bmp' %(msrc_directory+'/test',x) for x in self.img_list_test]\n",
    "        # self.x['train'] = ['%s/%s.bmp' %(msrc_directory,x) for x in self.img_list_train_val[:168]]\n",
    "        # self.y['train'] = ['%s/%s_GT.bmp' %(msrc_directory,x) for x in self.img_list_train_val[:168]]\n",
    "        # self.x['val'] = ['%s/%s.bmp' %(msrc_directory,x) for x in self.img_list_train_val[168:]]\n",
    "        # self.y['val'] = ['%s/%s_GT.bmp' %(msrc_directory,x) for x in self.img_list_train_val[168:]]\n",
    "        # self.x['test'] = ['%s/%s.bmp' %(msrc_directory,x) for x in self.img_list_test]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x[self.mode])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "      if self.mode in ['train', 'val']:\n",
    "          img = Image.open(self.x[self.mode][index]).convert('RGB')\n",
    "          #img = cv2.imread(self.x[self.mode][index]).astype(np.int)\n",
    "          mask = get_binary_seg(np.array(Image.open(self.y[self.mode][index]).convert('RGB')))#.astype(np.int)\n",
    "          mask = np.squeeze(mask.astype(np.uint8), axis=2)\n",
    "          mask = Image.fromarray(mask)\n",
    "          #mask = cv2.imread(self.y[self.mode][index])\n",
    "          #if (img.size[0]!=213):\n",
    "          #    img = img.transpose()\n",
    "          #    mask = cv2.transpose(mask)\n",
    "          tensor_img = self.transform(img)\n",
    "          tensor_mask = self.transform(mask)\n",
    "          return tensor_img,tensor_mask\n",
    "      else:\n",
    "          #img = cv2.imread(self.x[self.mode][index]).astype(np.float32)\n",
    "          img = Image.open(self.x[self.mode][index]).convert('RGB')\n",
    "          #if (img.size[0]!=213):\n",
    "          #    img = cv2.transpose(img)\n",
    "          tensor_img = self.transform(img)\n",
    "          return tensor_img\n",
    "\n",
    "def preprocessing():\n",
    "    img_transform = transforms.Compose(\n",
    "        [transforms.Resize((256, 256)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    mask_transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
    "    train_set = SegmentationData(img_transform=img_transform, mask_transform=mask_transform, mode='train')\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "    val_set = SegmentationData(img_transform=img_transform, mask_transform=mask_transform, mode='val')\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_set, batch_size=16, shuffle=False)\n",
    "    test_set = SegmentationData(img_transform=img_transform, mask_transform=mask_transform, mode='test')\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=16, shuffle=False)\n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5I2L8_zGmuy"
   },
   "source": [
    "For convenience, here's an example of how to use these dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "B-UblvceGpji",
    "outputId": "6c255850-d974-4651-de23-ed466b0ddc12",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "train_dataloader, val_dataloader, test_dataloader = preprocessing()\n",
    "input,labels = next(iter(train_dataloader))\n",
    "print(input.shape,labels.shape)\n",
    "print(type(input[2]))\n",
    "img = input[2].numpy().transpose(1, 2, 0)\n",
    "mask = labels[2].numpy().transpose(1, 2, 0)\n",
    "plot_image(img, 'train_image', cv2=False)\n",
    "plot_image(mask, 'train_seg')\n",
    "\n",
    "input,labels = next(iter(val_dataloader))\n",
    "print(input.shape,labels.shape)\n",
    "img = input[2].numpy().transpose(1, 2, 0)\n",
    "mask = labels[2].numpy().transpose(1, 2, 0)\n",
    "plot_image(img, 'val_image', cv2=False)\n",
    "plot_image(mask, 'val_seg')\n",
    "\n",
    "input = next(iter(test_dataloader))\n",
    "print(input.shape)\n",
    "img = input[2].numpy().transpose(1, 2, 0)\n",
    "plot_image(img, 'test_image', cv2=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ULv7QrwIvph"
   },
   "source": [
    "## 1. Architecture of your model\n",
    "\n",
    "Now that you are familiar with the dataset, it is time to build a deep neural network to perform these segmentations, where we need to distinguish foreground from background, where the class of interest is considered foreground.\n",
    "\n",
    "###  U-Net\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Alan-Jackson-2/publication/323597886/figure/fig2/AS:601386504957959@1520393124691/Convolutional-neural-network-CNN-architecture-based-on-UNET-Ronneberger-et-al.png\" style=\"width:650px;height:400px;\">\n",
    "\n",
    "A U-Net is an end-to-end segmentation network that should work reasonably well in this low data setting. It will take an image, progressively convolve it to a collection of many small feature maps, and then progressively up-convolve the maps while combining with crops from the previous layers. The figure provided gives an excellent example of a baseline U-Net that you can use as a starting point.\n",
    "\n",
    "For these operations, use ```nn.Conv2d```, ```torch.cat```, and ```nn.ConvTranspose2d```, ```nn.MaxPool2d```. You may find it useful to use ```nn.BatchNorm2d``` as well.\n",
    "\n",
    "Note that you can experiment with different channel sizes. Try to start with something smaller than 112, like 16.\n",
    "\n",
    "Let's now implement those the model!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8xp_64thIvph",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# define a convolution block which consist of Conv2d -> BatchNorm -> Relu\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, filter_size=3, stride = 1):\n",
    "        super().__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            # we are suppose to do \"same\" padding\n",
    "            nn.Conv2d(in_channels,out_channels, kernel_size=filter_size, padding=1, padding_mode='replicate', stride=stride),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x)\n",
    "\n",
    "\n",
    "# Define encoder block, which include max poll-> convBlock -> convBlock\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.encoder_block = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            ConvBlock(in_channels,out_channels),\n",
    "            ConvBlock(out_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder_block(x)\n",
    "\n",
    "\n",
    "# Define a deconvolution block, which is used to up-sampling the feature map.\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, conv_filter_size=3, conv_stride=1):\n",
    "        super().__init__()\n",
    "        self.deconv = nn.ConvTranspose2d(in_channels, in_channels //2, kernel_size=2, stride=2)\n",
    "        self.two_convs = nn.Sequential(\n",
    "            ConvBlock(in_channels, out_channels),\n",
    "            ConvBlock(out_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def padding(self, x1, x2):\n",
    "        diff_vertical = x2.size()[2] - x1.size()[2]\n",
    "        diff_horizontal = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diff_horizontal // 2, diff_horizontal - diff_horizontal // 2, diff_vertical // 2, diff_vertical - diff_vertical // 2])\n",
    "        return x1\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # pipeline: calculate up-sampling -> concat with prev layer -> 2 x conv\n",
    "        x1 = self.deconv(x1)\n",
    "        x1 = self.padding(x1, x2)\n",
    "        concat = torch.cat([x2, x1], 1)  # concat on dimension 1\n",
    "        result = self.two_convs(concat)\n",
    "        return result\n",
    "\n",
    "\n",
    "# Define last output layer: a conv layer with 1x1 filter size.\n",
    "class OutputLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutputLayer, self).__init__()\n",
    "        if out_channels == 1:\n",
    "            self.output = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size = 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        else:\n",
    "            self.output = nn.Conv2d(in_channels, out_channels, kernel_size = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, input_channel, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = input_channel\n",
    "        self.n_classes = n_classes\n",
    "        # define input layers\n",
    "        self.in_layer_1 = ConvBlock(self.n_channels, 64)\n",
    "        self.in_layer_2 = ConvBlock(64, 64)\n",
    "        # define encoder layers\n",
    "        self.encoder1 = EncoderBlock(64, 128)\n",
    "        self.encoder2 = EncoderBlock(128, 256)\n",
    "        self.encoder3 = EncoderBlock(256, 512)\n",
    "        self.encoder4 = EncoderBlock(512, 1024)\n",
    "        # define decoder layers\n",
    "        self.decoder1 = DecoderBlock(1024, 512)\n",
    "        self.decoder2 = DecoderBlock(512, 256)\n",
    "        self.decoder3 = DecoderBlock(256, 128)\n",
    "        self.decoder4 = DecoderBlock(128, 64)\n",
    "        # define output layers\n",
    "        self.out_layer = OutputLayer(64, self.n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input layer\n",
    "        x = self.in_layer_1(x)\n",
    "        i = self.in_layer_2(x)\n",
    "        # downsampling / conv\n",
    "        e1 = self.encoder1(i)\n",
    "        e2 = self.encoder2(e1)\n",
    "        e3 = self.encoder3(e2)\n",
    "        e4 = self.encoder4(e3)\n",
    "        # concat prev encoder layer. i.e. skip connection\n",
    "        # name the output as x to save graphic memory\n",
    "        x = self.decoder1(e4, e3)\n",
    "        x = self.decoder2(x, e2)\n",
    "        x = self.decoder3(x, e1)\n",
    "        x = self.decoder4(x, i)\n",
    "        logits = self.out_layer(x)\n",
    "        return logits"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLQVTm7kIvpi"
   },
   "source": [
    "## 2. Defining the for loop for train and validation phase\n",
    "\n",
    "### In each the phases certain things one has to be careful of:\n",
    "\n",
    "- Training Phase:\n",
    "  - Make sure the model is in train mode. That is ensured by `model.train()`\n",
    "\n",
    "  - While looping over instances of a batch, make sure the graidents are always set to zero before calling the backward function. That's done by `optim.zero_grad()`. If this is not done, the gradients get accumulated.\n",
    "\n",
    "  - Call the backward function on the loss by `loss.backward()` so that the loss get back propagated.\n",
    "\n",
    "  - Call the step function of the optimiser to update the weights of the network. This is done by `optim.step()`\n",
    "\n",
    "- Validation/Testing Phase\n",
    "  - Make sure your model is in eval mode. This makes the model deterministic rather than probabilistic. This is ensured by `model.eval()`\n",
    "  - As we don't need any gradients doing our validation/ testing phase, we can esnure that they are not calculated by defining a block with `torch.no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kyy8Hor_Ivpj",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "import util\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from dataloader import preprocessing\n",
    "from model import ConvBlock, EncoderBlock, DecoderBlock, OutputLayer, UNet\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from torchsummary import summary\n",
    "DEVICE = 'cuda:0'\n",
    "\n",
    "\n",
    "\n",
    "def training(train_dataloader, val_dataloader,\n",
    "             num_epochs=100,\n",
    "             lr=1e-4,\n",
    "             weight_decay=1e-8,\n",
    "             momentum=0.9,\n",
    "             batch_size=32):\n",
    "\n",
    "    # model initialization\n",
    "    number_of_class = 1\n",
    "    model = UNet(input_channel=3, n_classes=number_of_class)\n",
    "    model = model.cuda()\n",
    "    summary(model, (3, 256, 256))\n",
    "    criterion = nn.BCEWithLogitsLoss() if number_of_class == 1 else nn.CrossEntropyLoss()\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
    "    writer = SummaryWriter(comment=f'LR_{lr}_BS_{batch_size}')\n",
    "\n",
    "    # Run your training / validation loops\n",
    "    with tqdm(range(num_epochs), total=num_epochs, unit='Epoch') as pbar:\n",
    "        for epoch in pbar:\n",
    "            model.train()\n",
    "            # training loop\n",
    "            train_epoch_loss = 0\n",
    "            valid_epoch_loss = 0\n",
    "            train_epoch_acc = 0\n",
    "            valid_epoch_acc = 0\n",
    "            for batch_num, batch in enumerate(train_dataloader):\n",
    "                img, label = batch\n",
    "                img, label = img.to(DEVICE), label.to(DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                prediction = model(img)\n",
    "                if number_of_class != 1:\n",
    "                    train_loss = criterion(prediction, label.type(torch.int64).squeeze(1))\n",
    "                    # TODO calculate acc for instance segmentation\n",
    "                else:\n",
    "                    train_loss = criterion(prediction, label)\n",
    "                    train_acc = util.batch_accuracy(prediction, label)\n",
    "                train_epoch_loss += train_loss.item()\n",
    "                train_epoch_acc += train_acc\n",
    "                optimizer.zero_grad()\n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # validation loop\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                for batch_num, batch in enumerate(val_dataloader):\n",
    "                    img, label = batch\n",
    "                    img, label = img.to(DEVICE), label.to(DEVICE)\n",
    "                    prediction = model(img)\n",
    "                    if number_of_class != 1:\n",
    "                        val_loss = criterion(prediction, label.type(torch.int64).squeeze(1))\n",
    "                        # TODO calculate acc for instance segmentation\n",
    "                    else:\n",
    "                        val_loss = criterion(prediction, label)\n",
    "                        val_acc = util.batch_accuracy(prediction, label)\n",
    "                    valid_epoch_loss += val_loss.item()\n",
    "                    valid_epoch_acc  += val_acc\n",
    "            # Average accuracy\n",
    "            train_epoch_acc /= len(train_dataloader)\n",
    "            valid_epoch_acc /= len(train_dataloader)\n",
    "            pbar.set_postfix(train_loss=train_epoch_loss, train_acc=train_epoch_acc, val_loss=valid_epoch_loss, val_acc=valid_epoch_acc)\n",
    "            # write to tensorboard\n",
    "            writer.add_scalar('Loss/train', train_epoch_loss, epoch)\n",
    "            writer.add_scalar('Loss/validation', valid_epoch_loss, epoch)\n",
    "            writer.add_scalar('Accuracy/train', train_epoch_acc, epoch)\n",
    "            writer.add_scalar('Accuracy/validation', valid_epoch_acc, epoch)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define Hyper-parameters:\n",
    "num_epochs=100\n",
    "lr=1e-4\n",
    "weight_decay=1e-8\n",
    "momentum=0.9\n",
    "batch_size=32"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader = preprocessing()\n",
    "# util.dataloader_tester(train_dataloader, val_dataloader, test_dataloader)\n",
    "training(train_dataloader,val_dataloader,\n",
    "         num_epochs=num_epochs,\n",
    "             lr=lr,\n",
    "             weight_decay=weight_decay,\n",
    "             momentum=momentum,\n",
    "             batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wb3jjNrZlXDf"
   },
   "source": [
    "## 3. Challenge Submission\n",
    "Evaluate on the test set, and save the resulting segmentations in the same format as those in the initial dataset. \n",
    "Your challenge results should be saved in a torch file with the same format as in A3, with shape (24, 256, 256), where all values are either 1 (foreground) or 0 (background)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FQnzrR1qNz13",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FyDnVeQNn-V"
   },
   "source": [
    "Use this code to check your submission file:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-6hIbllAN3FU",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "masks = torch.load('results.pth.tar')\n",
    "\n",
    "assert(masks.shape == (24, 256, 256))\n",
    "assert((torch.where(masks == 1, 10, 0).sum() + torch.where(masks == 0, 10, 0).sum()).item() == 24 * 256 * 256 * 10)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qFesJ4VL1s_"
   },
   "source": [
    "## Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TECaZgpeL_9t"
   },
   "source": [
    "### 4. Plot training and validation loss per batch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rpO6r_GwL--S",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mj9GeO-uMMWI"
   },
   "source": [
    "### 5. Plot training and validation accuracy per epoch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HSeriL2sMT4f",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKZfb5ycMUP6"
   },
   "source": [
    "### 6. Show segmentation result for 3 test images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mQIBSAhLMY20",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}